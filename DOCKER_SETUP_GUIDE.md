# ðŸ³ Docker Setup Guide for D&D Story Telling

## ðŸš€ Quick Start Options

### ðŸ“± **Choose Your Deployment Method:**

1. **ðŸ–¥ï¸ Desktop/Server** - Windows/Linux/macOS with Docker Desktop
2. **ðŸ  Synology NAS** - DS718+ and compatible models ([Complete NAS Guide](docs/SYNOLOGY_NAS_DEPLOYMENT.md))
3. **â˜ï¸ Cloud Deployment** - AWS/Azure/GCP with container services
4. **ðŸ”§ Development** - Local development with hot reload

---

## ðŸ–¥ï¸ Desktop/Server Deployment

### Prerequisites
- Docker Desktop installed and running
- Docker Compose available
- Your API keys (configure below)
- 4GB+ RAM available
- 10GB+ disk space

### ðŸƒâ€â™‚ï¸ **1. Quick Start (Free Services)**

```bash
# Clone repository
git clone https://github.com/YOUR-USERNAME/DNDStoryTelling.git
cd DNDStoryTelling

# Copy environment template
cp .env.example .env

# Edit .env file with your OpenAI API key
# OPENAI_API_KEY=sk-your-key-here

# Start application (SQLite mode - no database setup needed)
docker-compose -f deployment/docker/docker-compose.free.yml up -d

# View logs
docker-compose -f deployment/docker/docker-compose.free.yml logs -f web

# Access application
# http://localhost:8001
```

### ðŸƒâ€â™‚ï¸ **2. Full Production Setup (PostgreSQL)**

```bash
# Navigate to the Docker directory
cd deployment/docker

# Start all services (web app + PostgreSQL database)
docker-compose up -d

# View logs
docker-compose logs -f web

# Stop all services
docker-compose down
```

### ðŸ  **3. Synology NAS Deployment**

For Synology DS718+ and compatible NAS devices, see our comprehensive guide:
ðŸ“– **[Complete NAS Deployment Guide](docs/SYNOLOGY_NAS_DEPLOYMENT.md)**

Quick overview:
- One-click container import
- Optimized for ARM64 architecture
- Resource-efficient configuration
- SQLite database (no PostgreSQL needed)
- Perfect for home labs

### ðŸŒ **2. Access Your Application**

- **Web Interface**: http://localhost:8001
- **Database**: PostgreSQL on port 5432
- **Redis** (optional): Port 6379 if using caching profile

### âš™ï¸ **3. Configuration**

Your API keys are already configured in the `.env` file:
- âœ… **OpenAI API Key**: Ready for GPT-4 and Whisper
- âœ… **Confluence Integration**: Connected to captains-log.atlassian.net
- âœ… **Database**: PostgreSQL with persistent storage

### ðŸ“‚ **4. Docker Architecture**

```
ðŸ³ Docker Container Setup:
â”œâ”€â”€ ðŸŒ Web Service (Port 8001)
â”‚   â”œâ”€â”€ FastAPI Application
â”‚   â”œâ”€â”€ Socket.IO for real-time chat
â”‚   â”œâ”€â”€ File upload handling
â”‚   â””â”€â”€ AI integration (OpenAI + Confluence)
â”œâ”€â”€ ðŸ—„ï¸ PostgreSQL Database (Port 5432)
â”‚   â”œâ”€â”€ User data storage
â”‚   â”œâ”€â”€ Session storage
â”‚   â””â”€â”€ Persistent volumes
â””â”€â”€ ðŸ“¦ Redis Cache (Optional, Port 6379)
    â””â”€â”€ Performance optimization
```

### ðŸ› ï¸ **5. Development Commands**

```bash
# Build and start services
docker-compose up --build

# Run in development mode with auto-reload
docker-compose -f docker-compose.yml up --build

# Access the container shell
docker-compose exec web bash

# View real-time logs
docker-compose logs -f

# Restart just the web service
docker-compose restart web

# Clean up everything (including volumes)
docker-compose down -v
```

### ðŸ” **6. Health Checks**

The Docker setup includes health checks:

```bash
# Check service status
docker-compose ps

# Test application health
curl http://localhost:8001/health

# Check database connection
docker-compose exec db pg_isready -U user -d dndstory
```

### ðŸ“Š **7. Monitoring & Logs**

```bash
# View all service logs
docker-compose logs

# Follow web application logs
docker-compose logs -f web

# View database logs
docker-compose logs db

# Check resource usage
docker stats
```

### ðŸ—‚ï¸ **8. Persistent Data**

Docker volumes ensure your data persists:
- **Database**: `postgres_data` volume
- **Uploads**: `app_uploads` volume
- **Logs**: `app_logs` volume
- **Redis Cache**: `redis_data` volume (if enabled)

### ðŸ”„ **9. Updates & Maintenance**

```bash
# Update application code
docker-compose pull
docker-compose up --build -d

# Backup database
docker-compose exec db pg_dump -U user dndstory > backup.sql

# Restore database
docker-compose exec -T db psql -U user dndstory < backup.sql
```

### ðŸ†š **10. Docker vs Local Development**

| Feature | Local Python | Docker |
|---------|-------------|---------|
| **Setup Time** | Longer (dependencies) | Quick (containerized) |
| **Database** | SQLite | PostgreSQL |
| **Isolation** | System-wide | Containerized |
| **Production Parity** | Lower | Higher |
| **Port** | 8000 | 8001 |
| **Performance** | Native | Slight overhead |

### ðŸŽ¯ **11. Testing Your Docker Setup**

1. **Start Services**: `docker-compose up -d`
2. **Check Health**: Visit http://localhost:8001
3. **Upload Test File**: Use your `test_dnd_session_detailed.txt`
4. **Test AI Features**: Verify OpenAI integration works
5. **Try Confluence**: Test export to your Atlassian workspace

### ðŸ› **12. Troubleshooting**

**Port Conflicts**:
```bash
# If port 8001 is in use, modify docker-compose.yml:
ports:
  - "8002:8000"  # Use port 8002 instead
```

**Database Issues**:
```bash
# Reset database
docker-compose down -v
docker-compose up -d
```

**Performance Issues**:
```bash
# Allocate more memory to Docker Desktop
# Settings â†’ Resources â†’ Advanced â†’ Memory (increase to 4GB+)
```

## âœ… **Ready to Go!**

Your Docker environment is configured with:
- ðŸ¤– **Full OpenAI Integration** (GPT-4 + Whisper)
- ðŸ¢ **Confluence Publishing** to captains-log.atlassian.net
- ðŸ—„ï¸ **PostgreSQL Database** with persistent storage
- ðŸš€ **Production-Ready Setup** with health checks and monitoring

**Start Command**: `docker-compose up -d`
**Access URL**: http://localhost:8001

Your D&D Story Telling application is ready to run in Docker! ðŸŽ²âœ¨